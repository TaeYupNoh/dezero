{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 역전파 이론\n",
    "\n",
    "- 역전파를 이용하면 미분을 효율적으로 계산할 수 있고 결괏값의 오차도 더 적음\n",
    "- 머신러닝을 주로 대량의 매개변수를 입력받아 마지막에 손실함수를 거쳐 출력을 내는 형태로 진행됨\n",
    "\n",
    "### 연쇄 법칙\n",
    "\n",
    "역전파를 이해하는 핵심 열쇠는 연쇄 법칙 **(Chain rule)** 입니다.\n",
    "\n",
    "chain은 **‘사슬’**이라는 뜻으로, 여러 함수를 사슬처럼 연결하여 사용하는 모습을 빗댄 것\n",
    "\n",
    "Chain rule에 따르면 합성 함수의 미분은 구성 함수 각각을 미분한 후 곱한 것과 같음\n",
    "\n",
    "- 합성 함수\n",
    "    \n",
    "    여러 함수가 연결된 함수\n",
    "    \n",
    "    ![그림 5-1.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f398948b-9349-4601-beec-16552e6c093a/%EA%B7%B8%EB%A6%BC_5-1.png)\n",
    "    \n",
    "    $$\n",
    "    y = F(x)\\;|\\;a=A(x),\\,b=B(a),\\,y=C(b)\n",
    "    $$\n",
    "    \n",
    "- 합성 함수의 미분\n",
    "    \n",
    "    $$\n",
    "    {dy\\over dx} = {dy\\over db}{db\\over da}{da\\over dx}\n",
    "    $$\n",
    "    \n",
    "    $x$에 대한 $y$의 미분은 구성 함수 각각의 미분 값을 모두 곱한 값과 같음\n",
    "    \n",
    "    따라서 합성 함수 $F$의 미분은 각 함수의 국소적인 미분들로 분해할 수 있음\n",
    "    \n",
    "    $$\n",
    "    {dy\\over dx} = {dy\\over dy}{dy\\over db}{db\\over da}{da\\over dx}\n",
    "    $$\n",
    "    \n",
    "    <aside>\n",
    "    💡 $dy\\over dx$는 $y$의 $y$에 대한 미분. 이 때 $y$가 작은 값만큼 변하면 자기 자신인 $y$도 당연히 같은 크기만큼 변함. 따라서 변화율은 어떤 함수의 경우에도 항상 1이 됨\n",
    "    \n",
    "    </aside>\n",
    "    \n",
    "\n",
    "### 역전파 원리 도출\n",
    "\n",
    "![그림 5-2.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/bf584721-4b9c-4387-bbf2-cb25f6eb1842/%EA%B7%B8%EB%A6%BC_5-2.png)\n",
    "\n",
    "출력 $y$에서 입력 $x$방향으로 곱하면서 순서대로 미분 하면 최종적으로 $dy\\over dx$가 구해짐.\n",
    "\n",
    "![그림 5-3.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d9bee566-2cb6-49aa-bc98-c45682521d7a/%EA%B7%B8%EB%A6%BC_5-3.png)\n",
    "\n",
    "![그림 5-4.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8a9ba6d3-2c2d-4751-9f32-473e9bf6255a/%EA%B7%B8%EB%A6%BC_5-4.png)\n",
    "\n",
    "위 그림을 보면 $y$의 각 변수에 대한 미분 값이 오른쪽에서 왼쪽으로 전파되는 것을 알 수 있음.\n",
    "\n",
    "이것이 **역**전파입니다. 여기서 중요한 점은 전파되는 데이터가 모두 $**y$의 미분값**이라는 것.\n",
    "\n",
    "구체적으로는 모두 $**y$의 ㅁㅁ에 대한 미분값**이 전파되고 있음.\n",
    "\n",
    "<aside>\n",
    "💡 계산 순서를 출력에서 입력 방향으로 정한 이유는 $y$의 미분 값을 전파하기 위함임.\n",
    "즉, $y$를 **중요 요소**로 대우하기 때문입니다. 만약 입력에서 출력 방향으로 계산했다면 중요 요소는 입력인 $x$가 됩니다. 이 경우 $x$에 대한 미분을 전파하게 됨.\n",
    "\n",
    "</aside>\n",
    "\n",
    "![x를 중요 요소로 볼 경우.](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2d019bfc-5ff5-4f52-beac-10d94afd7307/Untitled.png)\n",
    "\n",
    "x를 중요 요소로 볼 경우.\n",
    "\n",
    "머신러닝은 주로 대량의 매개변수를 입력 받아 마지막에 **손실 함수(Loss func)**를 거침.\n",
    "\n",
    "손실 함수의 출력은 (대체로) 단일한 스칼라이며, 이 값이 **중요 요소**.\n",
    "\n",
    "즉, **손실 함수의 각 매개변수에 대한 미분**을 계산해야 합니다. 이런 경우 미분 값을 출력에서 입력 방향으로 전파하면 한 번의 전파 만으로 모든 매개변수에 대한 미분을 계산할 수 있음.\n",
    "\n",
    "이러한 **효율성** 때문에 미분을 반대 방향으로 전파하는 역전파 방식을 이용하는 것.\n",
    "\n",
    "### 계산 그래프로 살펴보기\n",
    "\n",
    "![그림 5-5.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/59194b7e-a2f7-4752-841e-9074a9f4a798/%EA%B7%B8%EB%A6%BC_5-5.png)\n",
    "\n",
    "그림의 노드 $C'(b)$를 계산하려면 $b$값이 필요함. 마찬가지로 $B'(a)$를 구하려면 입력 $a$의 값이 필요함. **다시 말해, 역전파 시에는 순전파 시 이용한 데이터가 필요함.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
